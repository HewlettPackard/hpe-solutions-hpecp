###
## Copyright (2021) Hewlett Packard Enterprise Development LP
##
## Licensed under the Apache License, Version 2.0 (the "License");
## You may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
## http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.
####
---
###################### Servers #########################################

#Note: For NodeRole use appropriate roles for HPECP nodes
# CONTROLLER node: controller
# Gateway node: gateway
# master/worker nodes: clusternode1..n where n increases as the node count increases
# For Bonding_Interface1 and Bonding_Interface2 , enter the physical nic ports which will be used for NIC Teaming.
# Acceptable values for "GPU_Host" variable is "yes" for host which has GPU cards and "No" for hosts which do not have GPU cards.

servers:
 - Server_serial_number: "MXxxxxxDP"
   ILO_Address: 10.X.X.X
   ILO_Username: <username_of_the_admin_privileges_account>
   ILO_Password: <password_of_the_admin_privileges_account>
   Hostname: sles01.twentynet.local
   NodeRole: controller 
   Host_Username: root
   Host_Password: Password
   Bonding_Interface1: eth*
   Bonding_Interface2: eth*
   Host_IP: 20.X.X.X
   Host_Netmask: 255.X.X.X
   Host_Prefix: 8
   Host_Gateway: 20.X.X.X
   Host_DNS: 20.X.X.X
   Host_Search: twentynet.local
   GPU_Host: "yes"


 - Server_serial_number: "MXxxxxxDP"
   ILO_Address: 10.X.X.X
   ILO_Username: <username_of_the_admin_privileges_account>
   ILO_Password: <password_of_the_admin_privileges_account>
   Hostname: sles01.twentynet.local
   NodeRole: gateway 
   Host_Username: root
   Host_Password: Password
   Bonding_Interface1: eth*
   Bonding_Interface2: eth*
   Host_IP: 20.X.X.X
   Host_Netmask: 255.X.X.X
   Host_Prefix: 8
   Host_Gateway: 20.X.X.X
   Host_DNS: 20.X.X.X
   Host_Search: twentynet.local
   GPU_Host: "yes"
   
 - Server_serial_number: "MXxxxxxDP"
   ILO_Address: 10.X.X.X
   ILO_Username: <username_of_the_admin_privileges_account>
   ILO_Password: <password_of_the_admin_privileges_account>
   Hostname: sles01.twentynet.local
   NodeRole: clusternode1
   Host_Username: root
   Host_Password: Password
   Bonding_Interface1: eth*
   Bonding_Interface2: eth*
   Host_IP: 20.X.X.X
   Host_Netmask: 255.X.X.X
   Host_Prefix: 8
   Host_Gateway: 20.X.X.X
   Host_DNS: 20.X.X.X
   Host_Search: twentynet.local
   GPU_Host: "yes"

 - Server_serial_number: "MXxxxxxDP"
   ILO_Address: 10.X.X.X
   ILO_Username: <username_of_the_admin_privileges_account>
   ILO_Password: <password_of_the_admin_privileges_account>
   Hostname: sles01.twentynet.local
   NodeRole: clusternode2 
   Host_Username: root
   Host_Password: Password
   Bonding_Interface1: eth*
   Bonding_Interface2: eth*
   Host_IP: 20.X.X.X
   Host_Netmask: 255.X.X.X
   Host_Prefix: 8
   Host_Gateway: 20.X.X.X
   Host_DNS: 20.X.X.X
   Host_Search: twentynet.local
   GPU_Host: "yes"


 - Server_serial_number: "MXxxxxxDP"
   ILO_Address: 10.X.X.X
   ILO_Username: <username_of_the_admin_privileges_account>
   ILO_Password: <password_of_the_admin_privileges_account>
   Hostname: sles01.twentynet.local
   NodeRole: clusternode3
   Host_Username: root
   Host_Password: Password
   Bonding_Interface1: eth*
   Bonding_Interface2: eth*
   Host_IP: 20.X.X.X
   Host_Netmask: 255.X.X.X
   Host_Prefix: 8
   Host_Gateway: 20.X.X.X
   Host_DNS: 20.X.X.X
   Host_Search: twentynet.local
   GPU_Host: "yes"
   
######################## config.details ################################

 
config:
   HTTP_server_base_url: http://x.x.x.x/      #Installer machine ip address
   HTTP_file_path: /usr/share/nginx/html/
   OS_type: sles15
   OS_image_name: SLE-15-SP3-Full-x86_64-GM-Media1.iso
   base_kickstart_filepath: ./roles/os_deployment/tasks/autoinst.xml

 
#Mode 1 for RMT Registration and Mode 2 for User Registration


Mode: 2

Email: jim.****@abc.com           # User Email-id which will be used for server registration.
base_code: 88B***********         # Registration code which will be used for server registration.
ha_code: BD3FE***********         # Registration code for ha module

RMT_URL: http://20.**.**.**/

 

###Don't modify the sles_packages and zypper_pacakges as these packages are required for HPECP implementaion.Any changes made to this section may impact the installation.

 


sles_packages: [sle-module-public-cloud/15.3/x86_64,sle-module-legacy/15.3/x86_64,sle-module-python2/15.3/x86_64,sle-module-containers/15.3/x86_64,sle-module-basesystem/15.3/x86_64,PackageHub/15.3/x86_64,sle-module-desktop-applications/15.3/x86_64]
zypper_packages: [iputils,bind-utils,autofs,libcgroup1,kernel-default-devel,gcc-c++,rsyslog,firewalld,vim,perl,pciutils]


bundle_name: hpe-cp-bundle
tools_dir: "{{ lookup('env','HOME') }}/tools"                                               # place to store the kits like kubectl
k8skubeconfig_dir: "{{ lookup('env','HOME') }}/k8skubeconfig"
kubectl_cli_version: v1.18.6

credentials:
  ssh:
    username: <username>   # Update the variable with controller node username
    password: <password>   # Update the variable with controller node password

os: 
  value: False # For CentOS or RHEL set value as "True". For Sles set value as  "False".

common:
  platform: onprem  # No need to change
  #proxy:
  #  http: http://myproxy.com:8080
  #  https: http://myproxy.com:8080
  #  no_proxy: "localhost, 127.0.0.1, 10.1.10.0/24,172.20.0.0/16,10.244.0.0/16,10.96.0.0/16"
  name: OnPremSetup   # No need to change
  #certs:
  #  ssl_cert_file: /tmp/server.crt
  #  ssl_cert_key_file: /tmp/server.key

airgap:
  is_airgap_env: true  #set the value to true for deploying ECP in restricted environment otherwise false.
  feed_json_url: https://s3.amazonaws.com/bluedata-catalog/bundles/catalog/external/docker/CP-5.2/feeds/feed.json  # Make sure this url is pointing to t
  registry_server: testing_resgitry:8443 #  provide registry server ip and port number (ex:20.xx.xx.xx:5000)

init:
  host_ip: 20.x.x.x # provide controller node ip

controller:
  #url_bin_path: local:///tmp/hpe-cp-sles-release-5.2-3020.bin
  url_bin_path: https://my.s3.amazonaws.com/hpe-cp-bundle.bin # Make sure to change this url to correct bundle file or point to the local downloaded bin file path with executable permission ( ex:local:///root/hpe-cp-bundle.bin.)
  host_ip: 20.x.x.x # Provide controller node ip
  bd_domain: bddomain # No need to change
  bd_prefix: bluedata # No need to change
  hdfs_disks: /dev/xxx # Need to be empty when user want to deploy hpecp without tenant 
  no_tenant_isolation: false # In case if there is no requirement of using hdfs_disks, set this value to true.
  no_tenant_storage: false # In case if there is no requirement of using hdfs_disks, set this value to true
  node_disks: /dev/sdb # Provide which disk need to be used as node_disk for ephmeral storage
  int_start_ip: 172.20.0.2 # No need to change
  int_end_ip: 172.20.255.254  # No need to change
  int_gw_ip: 172.20.0.1  # No need to change
  int_nw_mask: 16   # No need to change
  #controller_public_if: ens192 # If multiple interfaces with IP address assignments found. Use --controller-public-if to specify a public interface.

controller_creds:
  name: admin # username to login to ere dashboard
  password: changeme # password to login to ere dashboard

# epic workers
epicworkers:   # Provide epic workers information. Epic worker details are mandatory in case if user want to enable controller ha
  host_pools:
    - name: worker1    # Name of the epic worker
      worker: 20.x.x.x
    - name: worker2      
      worker: 20.x.x.x
  hdfs_disks: /dev/xxx # Need to be empty when user want to deploy hpecp without tenant 
  node_disks: /dev/sdb # Provide which disk need to be used as node_disk for ephemeral  storage

gateway: # Provide gateway nodes information
  host_pools:
    - name: gateway1
      gateway: 20.x.x.x
      gateway_set_hostname: gatewayhpcpsles.jm.twentynet.local  # Host name of the gateway node
    - name: gateway2
      gateway: 20.x.x.x
      gateway_set_hostname: gatewayhpcpsles.jm.twentynet.local

controllerha:  # Provide cluster ip and FQDN name details for enabling controller ha
  clusterip: 20.x.x.x
  cluster_dns_name: vishha.jm.twentynet.local # Provide FQDN name. for example: cluster.local

# for k8s cluster
k8shosts: # Provide k8 nodes information
  host_pools:
    - name: master1   # Name of the node
      host: 20.x.x.x
      tagname: ""        # Provide tagname if it's required otherwise leave it as empty value. example:Datafabric,istio...etc
      tagvalue: ""        # Set the tagvalue to "yes" if the tagname is define otherwise leave it as empty value.
      persistent_disks: /dev/sdc  # Provide which disk need to be used as persistent disk
    - name: worker1
      host: 20.x.x.x
      tagname: "" ####empty for plain k8s, 1 for datafabric, 2 for rack,3 for istio-ingressgateway,4 for datacenter,5 for pensando,6 for falco
      tagvalue: ""##for datafabric value should be yes
      persistent_disks: /dev/sdc
    - name: worker2
      host: 20.x.x.x
      tagname: Datafabric
      tagvalue: yes
      persistent_disks: /dev/sdc
    - name: worker3
      host: 20.x.x.x
      tagname: Datafabric
      tagvalue: yes
      persistent_disks: /dev/sdc
    - name: worker4
      host: 20.x.x.x
      tagname: Datafabric
      tagvalue: yes
      persistent_disks: /dev/sdc
    - name: worker5
      host: 20.x.x.x
      tagname: Datafabric
      tagvalue: yes
  ephemeral_disks: /dev/sdb  # Provide which disk need to be used as ephermal_disk

  # Set persistent_disks value in case if user want to deploy "data fabric" storage cluster and if user have more disks , user can pass with comma (,) separated.
  # Set persistent_disks value empty in case if user want to deploy whit out tenant storage cluster i.e compute cluster.



k8scluster:  # Provide details of k8 cluster
  name: "cluster01"  # Name of the cluster
  description: "my first cluster"
  # TODO: get the host url through API
  host_pools:  # Provide role, ip, k8 version details
    - name: master-pool
      role: master    # Provide node role
      host: "20.x.x.x"
    - name: worker-pool
      role: worker
      host: "20.x.x.x"
    - name: worker-pool
      role: worker
      host: "20.x.x.x"
    - name: worker-pool
      role: worker
      host: "20.x.x.x"
    - name: worker-pool
      role: worker
      host: "20.x.x.x"
    - name: worker-pool
      role: worker
      host: "20.x.x.x"
  k8sversion: "1.23.10-hpe2"   # Provide k8s version
  pod_network_range: "10.244.0.0/16"
  service_network_range: "10.96.0.0/16"
  pod_dns_domain: "cluster.local"
  tagname: ""
  tagvalue: ""
  persistent_storage:
    nimble_csi: false
   

k8stenant:  # Provide tenant details.
  name: "test01"
  description: "my first tenant01"
  tenant_type_info:
    map_services_to_gateway: true
    specified_namespace_name: test01
    is_namespace_owner: true
    adopt_existing_namespace: false
  tenant_type: k8s
  features:
    ml_project: false
  is_namespace_owner: true
  quota:     # Provide quota details
    memory: 20480 # 20x1024
    persistent: 35840 # 35x1024
    gpus: ""
    cores: 12
    disk: 25600 # 25x1024
    tenant_storage: ""
  k8s_cluster: "cluster01"    # Provide cluster name
  specified_namespace_name: "test01" 
  map_services_to_gateway: true
  adopt_existing_namespace: false
  krb_enc_types: []

